# TitanUrl ğŸš€

A **production-grade URL shortening service** built with .NET 8, demonstrating high-scale distributed systems architecture with zero-collision ID generation, fire-and-forget analytics, and comprehensive observability.

## ğŸ¯ What Makes TitanUrl Different

Most URL shorteners use a **collision-check loop**:
```
1. Generate random string "abc"
2. Check DB: Does "abc" exist? â†’ YES
3. Generate "abd"
4. Check DB: Does "abd" exist? â†’ NO
5. Insert
```

**TitanUrl eliminates all collision checks:**
```
1. Generate ID 12345 (Snowflake - guaranteed unique)
2. Convert to "3d7" (Base62)
3. Insert directly â† ZERO database checks
```

This scales from 100s to **millions of requests/sec** without collision overhead.

---

## ğŸ—ï¸ Architecture

### **Core Components**

```
User Request (POST /api/shorten)
    â†“
Snowflake ID Generator (1 Î¼s, in-memory)
    â†“
Base62 Encoder (converts 64-bit â†’ 5-char string)
    â†“
Parallel Writes:
â”œâ”€â”€ PostgreSQL (durability)
â””â”€â”€ Redis (cache, 24h TTL)
    â†“
Response: {"shortCode": "k9J3z", "shortUrl": "http://localhost:5038/k9J3z"}
```

### **Read Path (with Fire-and-Forget Analytics)**

```
User Click GET /{shortCode}
    â†“
Two-tier Lookup:
â”œâ”€â”€ Redis Cache (1ms, ~87% hit rate)
â””â”€â”€ PostgreSQL (slow path, read repair)
    â†“
Fire Analytics Event (non-blocking):
â””â”€â”€ Write to Redis Stream "stream:clicks"
    â†“
Return 302 Redirect (user gets instant response)

(Parallel: AnalyticsWorker processes stream in background)
    â†“
Batch Aggregation:
â”œâ”€â”€ Read 100 events/batch
â”œâ”€â”€ Aggregate in-memory
â””â”€â”€ Single SQL UPSERT per unique URL
```

### **Key Design Patterns**

1. **Distributed ID Generation**: Twitter's Snowflake algorithm
   - 41-bit timestamp + 10-bit machine ID + 12-bit sequence
   - Guaranteed unique across distributed systems

2. **Compact Encoding**: Base62 (0-9, a-z, A-Z)
   - 64-bit integer â†’ ~11 characters (vs 20 for decimal)
   - Reversible (encode/decode without data loss)

3. **Separation of Concerns**:
   - User path (redirect) â‰  Analytics path (aggregation)
   - Non-blocking analytics via Redis Streams
   - Batch processing reduces database load by 100x

4. **Intelligent Caching**:
   - Read-after-write cache population
   - 24-hour TTL (configurable)
   - Cache miss â†’ read repair â†’ future cache hit

---

## ğŸš€ Quick Start

### Prerequisites
- .NET 8 SDK
- Docker & Docker Compose
- Git

### 1. Clone & Setup
```bash
git clone <repo>
cd TitanUrl
docker-compose up -d
```

### 2. Build & Run
```bash
dotnet build
dotnet run --project src/TitanUrl.Api/TitanUrl.Api.csproj
```

API available at: `http://localhost:5038`

### 3. Create a Short URL
```bash
curl -X POST http://localhost:5038/api/shorten \
  -H "Content-Type: application/json" \
  -d '{"url": "https://github.com/nawedx/TitanUrl"}'
```

**Response:**
```json
{
  "shortCode": "k9J3z",
  "shortUrl": "http://localhost:5038/k9J3z"
}
```

### 4. Click the Short URL
```bash
curl -L http://localhost:5038/k9J3z
```

Returns: HTTP 302 redirect to the original URL

---

## ğŸ“Š Observability Stack

TitanUrl includes **production-grade observability** with OpenTelemetry, Jaeger, and Prometheus.

### Access Points

| Tool | URL | Purpose |
|------|-----|---------|
| **Swagger** | http://localhost:5038/swagger | API documentation |
| **Health** | http://localhost:5038/health | K8s readiness probes |
| **Metrics** | http://localhost:5038/metrics | Prometheus scraping |
| **Jaeger UI** | http://localhost:16686 | Distributed tracing |
| **Prometheus** | http://localhost:9090 | Metrics querying |
| **Grafana** | http://localhost:3000 | Dashboards (admin/admin) |

### Custom Metrics

**AnalyticsWorker Metrics:**
- `clicks_processed_total` - Total clicks batch processed
- `worker_batch_size` - Size of each batch (1-100)
- `stream_lag` - Pending messages in Redis Stream (alerts when > 1000)

**Request Metrics:**
- `http_server_request_duration_seconds` - Request latency (auto-instrumented)
- Cache hit/miss rates (tracked via correlation IDs)

### Distributed Tracing

Every request gets a unique `correlation_id` that flows through:
1. HTTP request span
2. Redis operations
3. PostgreSQL queries
4. Analytics events

View in Jaeger to see the complete request journey.

---

## ğŸ“¡ API Endpoints

### Create Short URL
```
POST /api/shorten
Content-Type: application/json

{
  "url": "https://example.com/very/long/path?with=params&and=more"
}
```

**Response (201 Created):**
```json
{
  "shortCode": "k9J3z",
  "shortUrl": "http://localhost:5038/k9J3z"
}
```

### Follow Short URL
```
GET /{shortCode}
```

**Response (302 Found):**
```
Location: https://original-url.com/...
```

### Health Check
```
GET /health
```

**Response (200 OK):**
```json
{
  "status": "Healthy",
  "checks": {
    "redis": "Healthy",
    "postgres": "Healthy"
  }
}
```

### Metrics Endpoints (Development)
```
GET /api/metrics/stream-lag      # Redis Stream diagnostics
GET /api/metrics/redis-info      # Redis connection status
GET /api/metrics/postgres-status # PostgreSQL connection status
```

---

## ğŸ—„ï¸ Database Schema

### `url_mappings` - Core URL Storage
```sql
CREATE TABLE url_mappings (
    id BIGINT PRIMARY KEY,              -- Snowflake ID (unique, sortable)
    original_url TEXT NOT NULL,          -- Full URL to redirect to
    short_code VARCHAR(20) NOT NULL,     -- Base62-encoded short code
    created_at TIMESTAMP DEFAULT NOW()   -- Creation timestamp
);
CREATE INDEX idx_short_code ON url_mappings(short_code);
```

### `url_analytics` - Click Statistics
```sql
CREATE TABLE url_analytics (
    url_id BIGINT PRIMARY KEY,           -- Foreign key to url_mappings.id
    click_count BIGINT DEFAULT 0,        -- Total clicks
    last_updated TIMESTAMP DEFAULT NOW() -- Last batch update time
);
```

**Why separate tables?**
- `url_mappings`: Hot reads (every redirect)
- `url_analytics`: Batch writes (background worker)
- Separation prevents lock contention

---

## ğŸ§ª Testing

### Run All Tests
```bash
dotnet test
```

### Run Specific Test
```bash
dotnet test --filter "FullyQualifiedName~SnowflakeTests.Generate_ShouldReturnUniqueIds_SingleThread"
```

### Test Coverage
```bash
dotnet test /p:CollectCoverage=true
```

**Current Coverage:**
- Snowflake ID generation (uniqueness, monotonic ordering, bit extraction)
- Base62 encoding/decoding (roundtrip integrity, edge cases)

---

## ğŸ”§ Development Commands

```bash
# Build
dotnet build

# Run
dotnet run --project src/TitanUrl.Api/TitanUrl.Api.csproj

# Test
dotnet test

# Start infrastructure
docker-compose up -d

# Stop infrastructure
docker-compose down

# View logs
docker-compose logs -f
```

---

## ğŸ“ˆ Performance Characteristics

### Write Path (Shorten)
- Snowflake generation: ~0.1ms (in-memory)
- Base62 encoding: ~0.1ms
- PostgreSQL insert: ~2-5ms
- Redis cache: ~1ms
- **Total: ~5-10ms (no collisions!)**

### Read Path (Redirect)
- Cache hit: ~1ms (most requests)
- Cache miss + DB: ~5-10ms
- Analytics fire-and-forget: ~0.5ms
- **Total user wait: ~1-10ms (analytics non-blocking)**

### Analytics Worker
- Batch read (100 events): ~2ms
- In-memory aggregation: ~1ms
- Single SQL transaction: ~10-20ms
- **Throughput: ~5,000-10,000 clicks/sec per worker**

---

## ğŸš€ Production Deployment

### Kubernetes Readiness
- Health checks at `/health` (liveness + readiness)
- Metrics exposed at `/metrics` for Prometheus
- Distributed tracing via OpenTelemetry
- Environment-based configuration (connection strings)

### Scaling Strategy

**Horizontal Scaling:**
1. Multiple API instances (stateless)
   - Each gets unique `machineId` in Snowflake config
   - 10-bit machine ID supports 1,024 instances

2. Multiple AnalyticsWorker instances
   - Redis Streams consumer groups handle coordination
   - Automatic load balancing

3. Database sharding (if >100M URLs)
   - Shard on URL ID (first 8 bits = shard)
   - Each shard handles 2^56 IDs

### Monitoring & Alerts

Set up in Grafana/Prometheus:
```promql
# Alert: Worker falling behind
alert: stream_lag > 1000 for 5m

# Alert: High error rate
alert: rate(http_requests_total{status=~"5.."}[5m]) > 0.01

# Alert: Cache performance degrading
alert: cache_hit_rate < 0.70 for 10m
```

---

## ğŸ› ï¸ Project Structure

```
TitanUrl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ TitanUrl.Core/              # Business logic (no dependencies)
â”‚   â”‚   â”œâ”€â”€ SnowflakeIdGenerator.cs # Distributed ID generation
â”‚   â”‚   â””â”€â”€ Base62Converter.cs      # Compact encoding
â”‚   â””â”€â”€ TitanUrl.Api/               # ASP.NET Core API
â”‚       â”œâ”€â”€ Controllers/
â”‚       â”‚   â”œâ”€â”€ UrlController.cs    # POST /api/shorten
â”‚       â”‚   â”œâ”€â”€ RedirectController.cs # GET /{shortCode}
â”‚       â”‚   â””â”€â”€ MetricsController.cs  # Diagnostics
â”‚       â”œâ”€â”€ Workers/
â”‚       â”‚   â””â”€â”€ AnalyticsWorker.cs  # Background aggregation
â”‚       â”œâ”€â”€ Models/
â”‚       â”‚   â””â”€â”€ UrlDto.cs           # Request/response DTOs
â”‚       â””â”€â”€ Program.cs              # Configuration & setup
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ TitanUrl.Core.Tests/        # Unit tests
â”œâ”€â”€ docker-compose.yml              # Services: Postgres, Redis, Jaeger, Prometheus, Grafana
â”œâ”€â”€ prometheus.yml                  # Metrics scraping config
â””â”€â”€ README.md                       # This file
```

---

## ğŸ“ Learning Resources

### Key Concepts Demonstrated

1. **Distributed Systems**
   - Snowflake ID generation (Twitter's algorithm)
   - Distributed tracing (correlation IDs)
   - Event streaming (Redis Streams)

2. **High-Scale Architecture**
   - Fire-and-forget pattern (non-blocking analytics)
   - Batch aggregation (reduce database load)
   - Two-tier caching (Redis + DB)
   - Separation of concerns (user path â‰  analytics path)

3. **Observability**
   - OpenTelemetry instrumentation
   - Distributed tracing with Jaeger
   - Metrics with Prometheus
   - Custom dashboards with Grafana

4. **Database Design**
   - Snowflake IDs as primary keys (sortable, indexed)
   - UPSERT for analytics (INSERT OR UPDATE)
   - Index strategy (short_code for fast lookups)

---

## ğŸ“ License

MIT License - See LICENSE file for details

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Quality
- All new features must have unit tests
- Follow existing code style (C# conventions)
- Run `dotnet build` before committing
- Update README if adding new endpoints

---

## ğŸ“§ Support

For issues, questions, or suggestions:
- Open a GitHub Issue
- Check existing issues first
- Include reproduction steps for bugs

---

## ğŸ‰ Acknowledgments

Built as a demonstration of:
- Production-grade .NET application design
- High-scale distributed systems patterns
- Industry best practices for observability
- Clean architecture principles

**Special Thanks:**
- Twitter for Snowflake ID algorithm
- OpenTelemetry community
- .NET ecosystem contributors

---

**TitanUrl** - Building the future of URL shortening, one distributed ID at a time. ğŸš€
